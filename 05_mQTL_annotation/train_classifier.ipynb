{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Classifier to assign causal genes (UKB NMR 500K GWAS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "classifier_dataset = pd.read_csv(\n",
    "    \"classifier_dataset.tsv\",\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "subset_classifier_cholesterol = pd.read_csv(\n",
    "    \"classifier_dataset_cholesterol.tsv\",\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "subset_classifier_amino_acid = pd.read_csv(\n",
    "    \"classifier_dataset_amino_acid.tsv\",\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "subset_classifier_lipid = pd.read_csv(\n",
    "    \"classifier_dataset_lipid.tsv\",\n",
    "    sep=\"\\t\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess datasets \n",
    "- Filter variant gene pairs\n",
    "- remove overlapping variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy datasets\n",
    "cholesterol_dataset = subset_classifier_cholesterol.copy()\n",
    "amino_acid_dataset = subset_classifier_amino_acid.copy()\n",
    "lipid_dataset = subset_classifier_lipid.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove converging splits between the three datasets\n",
    "\n",
    "# rsids in cholesterol dataset 547\n",
    "ids_cholesterol_subset = cholesterol_dataset[\"variant_id\"].unique()\n",
    "\n",
    "# rsids in amino acid dataset 696\n",
    "ids_amino_acid_subset = amino_acid_dataset[\"variant_id\"].unique()\n",
    "\n",
    "# rsids in lipid dataset 704\n",
    "ids_lipid_subset = lipid_dataset[\"variant_id\"].unique()\n",
    "\n",
    "\n",
    "# goal ~ 775 per id,\n",
    "# overlap\n",
    "overlap_cholesterol_lipid = (\n",
    "    pd.Series(np.intersect1d(ids_cholesterol_subset, ids_lipid_subset))\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Split the sampled ids into two groups\n",
    "rsid_overlap_for_lipids = overlap_cholesterol_lipid.iloc[:71]\n",
    "rsid_overlap_for_cholesterol = overlap_cholesterol_lipid.iloc[71:]\n",
    "\n",
    "# divide\n",
    "cholesterol_dataset = cholesterol_dataset[\n",
    "    ~cholesterol_dataset[\"variant_id\"].isin(rsid_overlap_for_lipids)\n",
    "]\n",
    "lipid_dataset = lipid_dataset[\n",
    "    ~lipid_dataset[\"variant_id\"].isin(rsid_overlap_for_cholesterol)\n",
    "]\n",
    "\n",
    "# check that no overlap remains\n",
    "common_rsIDs = np.intersect1d(\n",
    "    cholesterol_dataset[\"variant_id\"], lipid_dataset[\"variant_id\"]\n",
    ")\n",
    "assert len(common_rsIDs) == 0, \"There are overlapping variant_ids in the datasets\"\n",
    "\n",
    "## now amino acids: just drop all rsids which are overlapping between the two datasets\n",
    "amino_acid_dataset = amino_acid_dataset[\n",
    "    ~amino_acid_dataset[\"variant_id\"].isin(\n",
    "        ids_cholesterol_subset.tolist() + ids_lipid_subset.tolist()\n",
    "    )\n",
    "]\n",
    "\n",
    "# Step 1: Find unique IDs in the first two datasets\n",
    "unique_ids_first_two = np.union1d(\n",
    "    cholesterol_dataset[\"variant_id\"], lipid_dataset[\"variant_id\"]\n",
    ")\n",
    "\n",
    "# Step 2: Find common IDs between the unique IDs of the first two and the third dataset\n",
    "common_ids_all = np.intersect1d(unique_ids_first_two, amino_acid_dataset[\"variant_id\"])\n",
    "\n",
    "# Step 3: Assert no common IDs exist\n",
    "assert len(common_ids_all) == 0, \"There are overlapping variant_ids among the datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def encode_columns(dataset, columns, pre_existing_encoders=None):\n",
    "    \"\"\"\n",
    "    Encodes specified columns of a dataset using LabelEncoder, with an option to use pre-existing encoders.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: pandas DataFrame to be modified.\n",
    "    - columns: List of column names in the dataset to be encoded.\n",
    "    - pre_existing_encoders: Optional; Dictionary of pre-existing LabelEncoders for specified columns.\n",
    "\n",
    "    Returns:\n",
    "    - Modified dataset with specified columns encoded.\n",
    "    - Dictionary of used encoders for the specified columns.\n",
    "\n",
    "    \"\"\"\n",
    "    if pre_existing_encoders is None:\n",
    "        pre_existing_encoders = {}  # Initialize if no encoders are provided\n",
    "\n",
    "    encoders = pre_existing_encoders.copy()  # Copy to avoid modifying the original dict\n",
    "    dataset_copy = dataset.copy()\n",
    "    for column in columns:\n",
    "        if column in encoders:\n",
    "            # Use the pre-existing encoder for this column\n",
    "            encoder = encoders[column]\n",
    "            dataset_copy[column] = encoder.transform(dataset_copy[column])\n",
    "        else:\n",
    "            # Create a new encoder, fit it, and transform the data\n",
    "            encoder = LabelEncoder()\n",
    "            dataset_copy[column] = encoder.fit_transform(dataset_copy[column])\n",
    "            encoders[column] = encoder  # Store the new encoder\n",
    "\n",
    "    return dataset_copy, encoders\n",
    "\n",
    "\n",
    "# Use label encoding:\n",
    "# encode the classifier dataset first to use this encoder on all subsets\n",
    "columns_to_encode = [\n",
    "    \"ensembl_id\",\n",
    "    \"variant_id\",\n",
    "    \"gene_biotype\",\n",
    "    \"hgnc_symbol\",\n",
    "    \"consequence_lead\",\n",
    "    \"consequence_proxy\",\n",
    "    \"IMPACT_lead\",\n",
    "    \"IMPACT_proxy\",\n",
    "]\n",
    "classifier_dataset_encoded, classifier_dataset_encoders = encode_columns(\n",
    "    classifier_dataset, columns_to_encode\n",
    ")\n",
    "# apply existing encoder on all subsets\n",
    "cholesterol_dataset, cholesterol_encoders = encode_columns(\n",
    "    cholesterol_dataset,\n",
    "    columns_to_encode,\n",
    "    pre_existing_encoders=classifier_dataset_encoders,\n",
    ")\n",
    "amino_acid_dataset, amino_acid_encoders = encode_columns(\n",
    "    amino_acid_dataset,\n",
    "    columns_to_encode,\n",
    "    pre_existing_encoders=classifier_dataset_encoders,\n",
    ")\n",
    "lipid_dataset, lipid_encoders = encode_columns(\n",
    "    lipid_dataset, columns_to_encode, pre_existing_encoders=classifier_dataset_encoders\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    balanced_accuracy_score,\n",
    "    cohen_kappa_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    cross_val_score,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf training function\n",
    "def random_forest_grouped_cv_with_tuning(\n",
    "    data,\n",
    "    exclude,\n",
    "    target,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=None,\n",
    "    param_grid=None,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    holdout_size=0.3,\n",
    "    group_column=\"variant_id\",\n",
    "):\n",
    "    # Initial split: training set (for tuning) and holdout set (for final evaluation)\n",
    "    X, y = data.drop(columns=exclude + [target]), data[target]\n",
    "    groups = data[group_column]\n",
    "    gss = GroupShuffleSplit(test_size=0.3, random_state=None)\n",
    "    for train_index, holdout_index in gss.split(X, y, groups=groups):\n",
    "        X_train, X_holdout = X.iloc[train_index], X.iloc[holdout_index]\n",
    "        y_train, y_holdout = y.iloc[train_index], y.iloc[holdout_index]\n",
    "        groups_train, groups_holdout = (\n",
    "            groups.iloc[train_index],\n",
    "            groups.iloc[holdout_index],\n",
    "        )\n",
    "        break\n",
    "\n",
    "    assert set(groups_train).isdisjoint(\n",
    "        set(groups_holdout)\n",
    "    ), \"Training and holdout groups have overlapping elements.\"\n",
    "\n",
    "    # Initialize the Random Forest classifier\n",
    "    clf = RandomForestClassifier(\n",
    "        random_state=random_state, class_weight=\"balanced\", n_jobs=n_jobs\n",
    "    )\n",
    "\n",
    "    # If no param_grid is provided, use this one\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            \"max_depth\": [80, 90, 100, 110],\n",
    "            \"max_features\": [2, 3],\n",
    "            \"min_samples_leaf\": [3, 4, 5],\n",
    "            \"min_samples_split\": [8, 10, 12],\n",
    "            \"n_estimators\": [100, 200, 300, 1000],\n",
    "        }\n",
    "\n",
    "    # Initialize GroupKFold\n",
    "    group_kfold = GroupKFold(n_splits=cv)\n",
    "\n",
    "    # Initialize GridSearchCV with GroupKFold\n",
    "    grid_search = GridSearchCV(\n",
    "        clf, param_grid, cv=group_kfold, n_jobs=n_jobs, scoring=scoring\n",
    "    )\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    grid_search.fit(X_train, y_train.values.ravel(), groups=groups_train)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions on the holdout set\n",
    "    predictions = best_model.predict(X_holdout)\n",
    "    predictions_prob = best_model.predict_proba(X_holdout)\n",
    "    predictions_prob_greater_05 = np.where(predictions_prob[:, 1] > 0.5, 1, 0).tolist()\n",
    "\n",
    "    # Calculate metrics\n",
    "    cm = confusion_matrix(y_holdout, predictions_prob_greater_05)\n",
    "    fpr, tpr, _ = roc_curve(y_holdout, predictions_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    precision, recall, _ = precision_recall_curve(y_holdout, predictions_prob[:, 1])\n",
    "    pr_auc = auc(recall, precision)\n",
    "    accuracy = accuracy_score(y_holdout, predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_holdout, predictions)\n",
    "    cohen_kappa = cohen_kappa_score(y_holdout, predictions)\n",
    "\n",
    "    # Cross-validated score of the best model\n",
    "    cv_scores = cross_val_score(\n",
    "        best_model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=group_kfold,\n",
    "        scoring=\"accuracy\",\n",
    "        groups=groups_train,\n",
    "    )\n",
    "\n",
    "    feature_importance = pd.Series(\n",
    "        best_model.feature_importances_, index=X_train.columns\n",
    "    )\n",
    "\n",
    "    # Return a dictionary containing the best model, its parameters, predictions, and metrics\n",
    "    return {\n",
    "        \"best_model\": best_model,\n",
    "        \"best_params\": grid_search.best_params_,\n",
    "        \"predictions\": predictions,\n",
    "        \"predictions_prob\": predictions_prob,\n",
    "        \"predictions_prob_greater_05\": predictions_prob_greater_05,\n",
    "        \"confusionMatrix\": cm,\n",
    "        \"roc_curve\": (fpr, tpr, roc_auc),\n",
    "        \"precision_recall_curve\": (precision, recall, pr_auc),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"balanced_accuracy\": balanced_accuracy,\n",
    "        \"cv_scores\": cv_scores,\n",
    "        \"mean_cv_score\": np.mean(cv_scores),\n",
    "        \"feature_importance\": feature_importance,\n",
    "        \"cohen_kappa\": cohen_kappa,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"y_holdout\": y_holdout,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with new param grid\n",
    "rf_clf_cholesterol = random_forest_grouped_cv_with_tuning(\n",
    "    cholesterol_dataset,\n",
    "    exclude=[\n",
    "        \"ensembl_id\",\n",
    "        \"variant_id\",\n",
    "        \"variant_id.hg19\",\n",
    "        \"hgnc_symbol\",\n",
    "        \"GO\",\n",
    "        \"KEGG\",\n",
    "        \"reactome\",\n",
    "        \"kegg_and_reactome_pathway_specific_gene\",\n",
    "        \"kegg_pathway_specific_gene\",\n",
    "        \"reactome_pathway_specific_gene\",\n",
    "        \"kegg_or_reactome_pathway_specific_gene\",\n",
    "        \"eric_fauman_egea_pair\",\n",
    "        \"eric_fauman_egea_gene\",\n",
    "    ],\n",
    "    target=\"kegg_or_reactome_pathway_specific_gene\",\n",
    "    param_grid=cholesterol_param_grid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the models for lipid and amino acid datasets\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cholesterol\n",
    "predictions_rf_cholesterol = rf_clf_cholesterol[\"best_model\"].predict_proba(\n",
    "    classifier_dataset_encoded.drop(\n",
    "        columns=[\n",
    "            \"ensembl_id\",\n",
    "            \"variant_id\",\n",
    "            \"variant_id.hg19\",\n",
    "            \"hgnc_symbol\",\n",
    "            \"GO\",\n",
    "            \"KEGG\",\n",
    "            \"reactome\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "# cholesterol\n",
    "predictions_rf_lipid = rf_clf_lipid[\"best_model\"].predict_proba(\n",
    "    classifier_dataset_encoded.drop(\n",
    "        columns=[\n",
    "            \"ensembl_id\",\n",
    "            \"variant_id\",\n",
    "            \"variant_id.hg19\",\n",
    "            \"hgnc_symbol\",\n",
    "            \"GO\",\n",
    "            \"KEGG\",\n",
    "            \"reactome\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "# amino acid\n",
    "predictions_rf_amino_acid = rf_clf_amino_acid[\"best_model\"].predict_proba(\n",
    "    classifier_dataset_encoded.drop(\n",
    "        columns=[\n",
    "            \"ensembl_id\",\n",
    "            \"variant_id\",\n",
    "            \"variant_id.hg19\",\n",
    "            \"hgnc_symbol\",\n",
    "            \"GO\",\n",
    "            \"KEGG\",\n",
    "            \"reactome\",\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add results to the decoded classifier dataset\n",
    "# take median of prediction scores \n",
    "\n",
    "classifier_dataset_w_predictions[\"prediction_median\"] = (\n",
    "    classifier_dataset_w_predictions[\n",
    "        [\n",
    "            \"predictions_rf_cholesterol_kegg_reactome_subset_on_cd\",\n",
    "            \"predictions_rf_lipid_kegg_reactome_subset_on_cd\",\n",
    "            \"predictions_rf_amino_acid_kegg_reactome_subset_on_cd\",\n",
    "        ]\n",
    "    ].median(axis=1)\n",
    ")\n",
    "# label in_training_set ids\n",
    "classifier_dataset_w_predictions[\"in_training_set\"] = classifier_dataset_w_predictions[\n",
    "    \"variant_id\"\n",
    "].apply(lambda x: 1 if x in all_ids_in_subsets else 0)\n",
    "\n",
    "idx = classifier_dataset_w_predictions.groupby(\"variant_id\")[\n",
    "    \"prediction_median\"\n",
    "].idxmax()\n",
    "# get the variant_id gene_pair with the highest median prediction score\n",
    "\n",
    "idx = classifier_dataset_w_predictions.groupby(\"variant_id\")[\n",
    "    \"prediction_median\"\n",
    "].idxmax()\n",
    "\n",
    "# Step 2: Create a new column and set it to 1 for these rows\n",
    "classifier_dataset_w_predictions[\"prediction_median_pick\"] = 0\n",
    "classifier_dataset_w_predictions.loc[idx, \"prediction_median_pick\"] = 1\n",
    "\n",
    "\n",
    "classifier_dataset_w_predictions.to_csv(\n",
    "    \"output/predctions_classifer.tsv\",\n",
    "    sep=\"\\t\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
